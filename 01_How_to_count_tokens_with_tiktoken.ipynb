{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeshkumar003/Generative_AI_Notebooks/blob/main/01_How_to_count_tokens_with_tiktoken.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-VQT3ju9eMM"
      },
      "source": [
        "# Assignment 2: Counting Tokens and Estimating Cost\n",
        "\n",
        "Example:\n",
        "https://tiktokenizer.vercel.app/?model=gpt-4-1106-preview\n",
        "\n",
        "## Objective:\n",
        "Write a Python program to count the number of tokens in a given text source (paragraph, text file, or PDF file) and calculate the cost of processing these tokens using OpenAI's GPT-4o pricing model.\n",
        "\n",
        "## Requirements:\n",
        "1. **Input**:\n",
        "   - A paragraph as a string.\n",
        "   - A text file containing the content.\n",
        "   - A PDF file with the content.\n",
        "2. **Output**:\n",
        "   - The total number of tokens in the input.\n",
        "   - The estimated cost of processing the input tokens using GPT-4o pricing ($2.50 per 1M tokens).\n",
        "3. **Constraints**:\n",
        "   - Use OpenAI's `tiktoken` library to tokenize the input.\n",
        "   - Ensure compatibility with different file formats (text and PDF).\n",
        "   - Handle invalid or empty inputs gracefully.\n",
        "\n",
        "## Example:\n",
        "### Input:\n",
        "```plaintext\n",
        "Paragraph: \"Tiktoken is a tokenizer by OpenAI. It splits text into tokens.\"\n",
        "```\n",
        "### Output:\n",
        "```plaintext\n",
        "Total Tokens: 12\n",
        "Estimated Cost: $0.00003\n",
        "```\n",
        "\n",
        "### Input:\n",
        "```plaintext\n",
        "Text File: \"example.txt\" (contains 500 tokens)\n",
        "```\n",
        "### Output:\n",
        "```plaintext\n",
        "Total Tokens: 500\n",
        "Estimated Cost: $0.00125\n",
        "```\n",
        "\n",
        "## Extra Credit:\n",
        "Whoever calculates the total number of tokens and the cost for all 6 **Harry Potter** books combined will receive an **assignment pass**, which can be used to skip any future assignment.\n",
        "\n",
        "### Additional Notes:\n",
        "- Use `tiktoken`'s `encode` method to calculate token counts.\n",
        "- For PDF files, extract the text content first (e.g., using a library like `PyPDF2`).\n",
        "- Costs should be calculated as `total_tokens / 1,000,000 * 2.50`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai6QcQDJ7uVi"
      },
      "source": [
        "# How to count tokens with tiktoken\n",
        "\n",
        "[`tiktoken`](https://github.com/openai/tiktoken/blob/main/README.md) is a fast open-source tokenizer by OpenAI.\n",
        "\n",
        "Given a text string (e.g., `\"tiktoken is great!\"`) and an encoding (e.g., `\"cl100k_base\"`), a tokenizer can split the text string into a list of tokens (e.g., `[\"t\", \"ik\", \"token\", \" is\", \" great\", \"!\"]`).\n",
        "\n",
        "Splitting text strings into tokens is useful because GPT models see text in the form of tokens. Knowing how many tokens are in a text string can tell you (a) whether the string is too long for a text model to process and (b) how much an OpenAI API call costs (as usage is priced by token).\n",
        "\n",
        "\n",
        "## Encodings\n",
        "\n",
        "Encodings specify how text is converted into tokens. Different models use different encodings.\n",
        "\n",
        "`tiktoken` supports three encodings used by OpenAI models:\n",
        "\n",
        "| Encoding name           | OpenAI models                                       |\n",
        "|-------------------------|-----------------------------------------------------|\n",
        "| `o200k_base`            | `gpt-4o`, `gpt-4o-mini`                             |\n",
        "| `cl100k_base`           | `gpt-4-turbo`, `gpt-4`, `gpt-3.5-turbo`, `text-embedding-ada-002`, `text-embedding-3-small`, `text-embedding-3-large`  |\n",
        "| `p50k_base`             | Codex models, `text-davinci-002`, `text-davinci-003`|\n",
        "| `r50k_base` (or `gpt2`) | GPT-3 models like `davinci`                         |\n",
        "\n",
        "You can retrieve the encoding for a model using `tiktoken.encoding_for_model()` as follows:\n",
        "```python\n",
        "encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
        "```\n",
        "\n",
        "Note that `p50k_base` overlaps substantially with `r50k_base`, and for non-code applications, they will usually give the same tokens.\n",
        "\n",
        "## Tokenizer libraries by language\n",
        "\n",
        "For `o200k_base`, `cl100k_base` and `p50k_base` encodings:\n",
        "- Python: [tiktoken](https://github.com/openai/tiktoken/blob/main/README.md)\n",
        "- .NET / C#: [SharpToken](https://github.com/dmitry-brazhenko/SharpToken), [TiktokenSharp](https://github.com/aiqinxuancai/TiktokenSharp)\n",
        "- Java: [jtokkit](https://github.com/knuddelsgmbh/jtokkit)\n",
        "- Golang: [tiktoken-go](https://github.com/pkoukk/tiktoken-go)\n",
        "- Rust: [tiktoken-rs](https://github.com/zurawiki/tiktoken-rs)\n",
        "\n",
        "For `r50k_base` (`gpt2`) encodings, tokenizers are available in many languages.\n",
        "- Python: [tiktoken](https://github.com/openai/tiktoken/blob/main/README.md) (or alternatively [GPT2TokenizerFast](https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.GPT2TokenizerFast))\n",
        "- JavaScript: [gpt-3-encoder](https://www.npmjs.com/package/gpt-3-encoder)\n",
        "- .NET / C#: [GPT Tokenizer](https://github.com/dluc/openai-tools)\n",
        "- Java: [gpt2-tokenizer-java](https://github.com/hyunwoongko/gpt2-tokenizer-java)\n",
        "- PHP: [GPT-3-Encoder-PHP](https://github.com/CodeRevolutionPlugins/GPT-3-Encoder-PHP)\n",
        "- Golang: [tiktoken-go](https://github.com/pkoukk/tiktoken-go)\n",
        "- Rust: [tiktoken-rs](https://github.com/zurawiki/tiktoken-rs)\n",
        "\n",
        "(OpenAI makes no endorsements or guarantees of third-party libraries.)\n",
        "\n",
        "\n",
        "## How strings are typically tokenized\n",
        "\n",
        "In English, tokens commonly range in length from one character to one word (e.g., `\"t\"` or `\" great\"`), though in some languages tokens can be shorter than one character or longer than one word. Spaces are usually grouped with the starts of words (e.g., `\" is\"` instead of `\"is \"` or `\" \"`+`\"is\"`). You can quickly check how a string is tokenized at the [OpenAI Tokenizer](https://beta.openai.com/tokenizer), or the third-party [Tiktokenizer](https://tiktokenizer.vercel.app/) webapp."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wendmGSL7uVk"
      },
      "source": [
        "## 0. Install `tiktoken`\n",
        "\n",
        "If needed, install `tiktoken` with `pip`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hNgxRDZa7uVl"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade tiktoken -q\n",
        "%pip install --upgrade openai -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuxBm3Os7uVm"
      },
      "source": [
        "## 1. Import `tiktoken`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bo-PN1gQ7uVm"
      },
      "outputs": [],
      "source": [
        "import tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIWYiQcl7uVm"
      },
      "source": [
        "## 2. Load an encoding\n",
        "\n",
        "Use `tiktoken.get_encoding()` to load an encoding by name.\n",
        "\n",
        "The first time this runs, it will require an internet connection to download. Later runs won't need an internet connection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aBKsV_BS7uVm"
      },
      "outputs": [],
      "source": [
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8te9XsO07uVn"
      },
      "source": [
        "Use `tiktoken.encoding_for_model()` to automatically load the correct encoding for a given model name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "X80VuaDU7uVn"
      },
      "outputs": [],
      "source": [
        "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XmCT7Cz7uVn"
      },
      "source": [
        "## 3. Turn text into tokens with `encoding.encode()`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-m2imT67uVn"
      },
      "source": [
        "The `.encode()` method converts a text string into a list of token integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMpqB1Jm7uVn",
        "outputId": "0fc57d0e-4d61-4371-de1b-1cd6552928a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[83, 8251, 2488, 382, 2212, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "encoding.encode(\"tiktoken is great!\")\n",
        "# encoding.encode(\"tiktoken\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js033ojb7uVn"
      },
      "source": [
        "Count tokens by counting the length of the list returned by `.encode()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "G3KpHVOw7uVn"
      },
      "outputs": [],
      "source": [
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qqtia3cP7uVo",
        "outputId": "09ea40c6-0254-431b-f693-ecc4754f440c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "num_tokens_from_string(\"tiktoken is great!\", \"o200k_base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snO2f70B7uVo"
      },
      "source": [
        "## 4. Turn tokens into text with `encoding.decode()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5letL4i7uVo"
      },
      "source": [
        "`.decode()` converts a list of token integers to a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tvpNAiE17uVo",
        "outputId": "6cf0ccde-403f-4d03-f111-7dc5c6308238"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tiktoken is great!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "encoding.decode([83, 8251, 2488, 382, 2212, 0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5m3do1u7uVo"
      },
      "source": [
        "Warning: although `.decode()` can be applied to single tokens, beware that it can be lossy for tokens that aren't on utf-8 boundaries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxdBHBuh7uVo"
      },
      "source": [
        "For single tokens, `.decode_single_token_bytes()` safely converts a single integer token to the bytes it represents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Nc-_cvZ7uVo",
        "outputId": "589f3ea3-7974-4403-e33a-b71adac907c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[b't', b'ikt', b'oken', b' is', b' great', b'!']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "[encoding.decode_single_token_bytes(token) for token in [83, 8251, 2488, 382, 2212, 0]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmlt5AmM7uVo"
      },
      "source": [
        "(The `b` in front of the strings indicates that the strings are byte strings.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXgvVoVf7uVo"
      },
      "source": [
        "## 5. Comparing encodings\n",
        "\n",
        "Different encodings vary in how they split words, group spaces, and handle non-English characters. Using the methods above, we can compare different encodings on a few example strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "z7EtgZno7uVo"
      },
      "outputs": [],
      "source": [
        "def compare_encodings(example_string: str) -> None:\n",
        "    \"\"\"Prints a comparison of three string encodings.\"\"\"\n",
        "    # print the example string\n",
        "    print(f'\\nExample string: \"{example_string}\"')\n",
        "    # for each encoding, print the # of tokens, the token integers, and the token bytes\n",
        "    for encoding_name in [\"r50k_base\", \"p50k_base\", \"cl100k_base\", \"o200k_base\"]:\n",
        "        encoding = tiktoken.get_encoding(encoding_name)\n",
        "        token_integers = encoding.encode(example_string)\n",
        "        num_tokens = len(token_integers)\n",
        "        token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integers]\n",
        "        print()\n",
        "        print(f\"{encoding_name}: {num_tokens} tokens\")\n",
        "        print(f\"token integers: {token_integers}\")\n",
        "        print(f\"token bytes: {token_bytes}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmIlYgHB7uVo",
        "outputId": "3c3b7cc6-94e9-47f4-bc63-0809542dc85a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example string: \"antidisestablishmentarianism\"\n",
            "\n",
            "r50k_base: 5 tokens\n",
            "token integers: [415, 29207, 44390, 3699, 1042]\n",
            "token bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n",
            "\n",
            "p50k_base: 5 tokens\n",
            "token integers: [415, 29207, 44390, 3699, 1042]\n",
            "token bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n",
            "\n",
            "cl100k_base: 6 tokens\n",
            "token integers: [519, 85342, 34500, 479, 8997, 2191]\n",
            "token bytes: [b'ant', b'idis', b'establish', b'ment', b'arian', b'ism']\n",
            "\n",
            "o200k_base: 6 tokens\n",
            "token integers: [493, 129901, 376, 160388, 21203, 2367]\n",
            "token bytes: [b'ant', b'idis', b'est', b'ablishment', b'arian', b'ism']\n"
          ]
        }
      ],
      "source": [
        "compare_encodings(\"antidisestablishmentarianism\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byM1D1bi7uVo",
        "outputId": "bf7a3dfd-3a85-4807-b909-4dc8a48f21ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example string: \"2 + 2 = 4\"\n",
            "\n",
            "r50k_base: 5 tokens\n",
            "token integers: [17, 1343, 362, 796, 604]\n",
            "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n",
            "\n",
            "p50k_base: 5 tokens\n",
            "token integers: [17, 1343, 362, 796, 604]\n",
            "token bytes: [b'2', b' +', b' 2', b' =', b' 4']\n",
            "\n",
            "cl100k_base: 7 tokens\n",
            "token integers: [17, 489, 220, 17, 284, 220, 19]\n",
            "token bytes: [b'2', b' +', b' ', b'2', b' =', b' ', b'4']\n",
            "\n",
            "o200k_base: 7 tokens\n",
            "token integers: [17, 659, 220, 17, 314, 220, 19]\n",
            "token bytes: [b'2', b' +', b' ', b'2', b' =', b' ', b'4']\n"
          ]
        }
      ],
      "source": [
        "compare_encodings(\"2 + 2 = 4\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cyr9NUa7uVp",
        "outputId": "aac92784-335b-4817-de3f-0881c5deb6e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example string: \"السلام علیکم ، کیسے ہیں آپ؟\"\n",
            "\n",
            "r50k_base: 36 tokens\n",
            "token integers: [23525, 45692, 13862, 12919, 25405, 17550, 117, 13862, 151, 234, 150, 102, 25405, 17550, 234, 220, 150, 102, 151, 234, 45692, 151, 240, 220, 151, 223, 151, 234, 150, 118, 17550, 95, 149, 122, 148, 253]\n",
            "token bytes: [b'\\xd8\\xa7\\xd9\\x84', b'\\xd8\\xb3', b'\\xd9\\x84', b'\\xd8\\xa7', b'\\xd9\\x85', b' \\xd8', b'\\xb9', b'\\xd9\\x84', b'\\xdb', b'\\x8c', b'\\xda', b'\\xa9', b'\\xd9\\x85', b' \\xd8', b'\\x8c', b' ', b'\\xda', b'\\xa9', b'\\xdb', b'\\x8c', b'\\xd8\\xb3', b'\\xdb', b'\\x92', b' ', b'\\xdb', b'\\x81', b'\\xdb', b'\\x8c', b'\\xda', b'\\xba', b' \\xd8', b'\\xa2', b'\\xd9', b'\\xbe', b'\\xd8', b'\\x9f']\n",
            "\n",
            "p50k_base: 36 tokens\n",
            "token integers: [23525, 45692, 13862, 12919, 25405, 17550, 117, 13862, 151, 234, 150, 102, 25405, 17550, 234, 220, 150, 102, 151, 234, 45692, 151, 240, 220, 151, 223, 151, 234, 150, 118, 17550, 95, 149, 122, 148, 253]\n",
            "token bytes: [b'\\xd8\\xa7\\xd9\\x84', b'\\xd8\\xb3', b'\\xd9\\x84', b'\\xd8\\xa7', b'\\xd9\\x85', b' \\xd8', b'\\xb9', b'\\xd9\\x84', b'\\xdb', b'\\x8c', b'\\xda', b'\\xa9', b'\\xd9\\x85', b' \\xd8', b'\\x8c', b' ', b'\\xda', b'\\xa9', b'\\xdb', b'\\x8c', b'\\xd8\\xb3', b'\\xdb', b'\\x92', b' ', b'\\xdb', b'\\x81', b'\\xdb', b'\\x8c', b'\\xda', b'\\xba', b' \\xd8', b'\\xa2', b'\\xd9', b'\\xbe', b'\\xd8', b'\\x9f']\n",
            "\n",
            "cl100k_base: 27 tokens\n",
            "token integers: [32482, 20665, 8700, 50488, 45082, 8700, 14728, 33411, 10386, 8979, 234, 46693, 14728, 20665, 151, 240, 220, 151, 223, 14728, 150, 118, 8979, 95, 68483, 148, 253]\n",
            "token bytes: [b'\\xd8\\xa7\\xd9\\x84', b'\\xd8\\xb3', b'\\xd9\\x84', b'\\xd8\\xa7\\xd9\\x85', b' \\xd8\\xb9', b'\\xd9\\x84', b'\\xdb\\x8c', b'\\xda\\xa9', b'\\xd9\\x85', b' \\xd8', b'\\x8c', b' \\xda\\xa9', b'\\xdb\\x8c', b'\\xd8\\xb3', b'\\xdb', b'\\x92', b' ', b'\\xdb', b'\\x81', b'\\xdb\\x8c', b'\\xda', b'\\xba', b' \\xd8', b'\\xa2', b'\\xd9\\xbe', b'\\xd8', b'\\x9f']\n",
            "\n",
            "o200k_base: 8 tokens\n",
            "token integers: [131610, 47103, 105106, 4281, 179928, 12406, 26418, 11388]\n",
            "token bytes: [b'\\xd8\\xa7\\xd9\\x84\\xd8\\xb3\\xd9\\x84\\xd8\\xa7\\xd9\\x85', b' \\xd8\\xb9\\xd9\\x84\\xdb\\x8c', b'\\xda\\xa9\\xd9\\x85', b' \\xd8\\x8c', b' \\xda\\xa9\\xdb\\x8c\\xd8\\xb3\\xdb\\x92', b' \\xdb\\x81\\xdb\\x8c\\xda\\xba', b' \\xd8\\xa2\\xd9\\xbe', b'\\xd8\\x9f']\n"
          ]
        }
      ],
      "source": [
        "compare_encodings(\"السلام علیکم ، کیسے ہیں آپ؟\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMcXOqlH7uVp"
      },
      "source": [
        "## 6. Counting tokens for chat completions API calls\n",
        "\n",
        "ChatGPT models like `gpt-4o-mini` and `gpt-4` use tokens in the same way as older completions models, but because of their message-based formatting, it's more difficult to count how many tokens will be used by a conversation.\n",
        "\n",
        "Below is an example function for counting tokens for messages passed to `gpt-3.5-turbo`, `gpt-4`, `gpt-4o` and `gpt-4o-mini`.\n",
        "\n",
        "Note that the exact way that tokens are counted from messages may change from model to model. Consider the counts from the function below an estimate, not a timeless guarantee.\n",
        "\n",
        "In particular, requests that use the optional functions input will consume extra tokens on top of the estimates calculated below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LLvS_AdI7uVp"
      },
      "outputs": [],
      "source": [
        "def num_tokens_from_messages(messages, model=\"gpt-4o-mini-2024-07-18\"):\n",
        "    \"\"\"Return the number of tokens used by a list of messages.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        print(\"Warning: model not found. Using o200k_base encoding.\")\n",
        "        encoding = tiktoken.get_encoding(\"o200k_base\")\n",
        "    if model in {\n",
        "        \"gpt-3.5-turbo-0125\",\n",
        "        \"gpt-4-0314\",\n",
        "        \"gpt-4-32k-0314\",\n",
        "        \"gpt-4-0613\",\n",
        "        \"gpt-4-32k-0613\",\n",
        "        \"gpt-4o-mini-2024-07-18\",\n",
        "        \"gpt-4o-2024-08-06\"\n",
        "        }:\n",
        "        tokens_per_message = 3\n",
        "        tokens_per_name = 1\n",
        "    elif \"gpt-3.5-turbo\" in model:\n",
        "        print(\"Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0125.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0125\")\n",
        "    elif \"gpt-4o-mini\" in model:\n",
        "        print(\"Warning: gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-mini-2024-07-18.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-4o-mini-2024-07-18\")\n",
        "    elif \"gpt-4o\" in model:\n",
        "        print(\"Warning: gpt-4o and gpt-4o-mini may update over time. Returning num tokens assuming gpt-4o-2024-08-06.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-4o-2024-08-06\")\n",
        "    elif \"gpt-4\" in model:\n",
        "        print(\"Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613.\")\n",
        "        return num_tokens_from_messages(messages, model=\"gpt-4-0613\")\n",
        "    else:\n",
        "        raise NotImplementedError(\n",
        "            f\"\"\"num_tokens_from_messages() is not implemented for model {model}.\"\"\"\n",
        "        )\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
        "    return num_tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJRUQEQP-yJW",
        "outputId": "3ee9b5ba-1e15-4d8b-c47c-a2598023ce80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens: 15\n",
            "Estimated Cost: $3.7500000000000003e-05\n",
            "Estimated Cost: $0.000038\n"
          ]
        }
      ],
      "source": [
        "def count_tokens_and_cost_with_messages(messages, model=\"gpt-4o-mini-2024-07-18\"):\n",
        "    \"\"\"\n",
        "    Counts the total tokens and calculates the processing cost for a list of messages.\n",
        "\n",
        "    Args:\n",
        "    - messages (list): A list of message dictionaries, where each dictionary represents a message.\n",
        "    - model (str): The model name used for token calculation (default: \"gpt-4o-mini-2024-07-18\").\n",
        "\n",
        "    Returns:\n",
        "    - dict: A dictionary containing the total tokens and estimated cost.\n",
        "    \"\"\"\n",
        "    # Calculate the total tokens using the existing method\n",
        "    total_tokens = num_tokens_from_messages(messages, model)\n",
        "\n",
        "    # Calculate the cost based on GPT-4o pricing\n",
        "    cost_per_million_tokens = 2.50  # $2.50 per 1M tokens\n",
        "    cost = (total_tokens / 1_000_000) * cost_per_million_tokens\n",
        "\n",
        "    # Return the result as a dictionary\n",
        "    return {\"Total Tokens\": total_tokens, \"Cost (USD)\": cost}\n",
        "\n",
        "# Example usage\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is the weather today in Karachi?\"}\n",
        "]\n",
        "\n",
        "result = count_tokens_and_cost_with_messages(messages)\n",
        "print(f\"Total Tokens: {result['Total Tokens']}\")\n",
        "print(f\"Estimated Cost: ${result['Cost (USD)']:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okRnIdoRfTHA",
        "outputId": "ba2ced6d-7ab7-4562-a625-365a384d55a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens: 187\n",
            "Estimated Cost: $0.000467\n"
          ]
        }
      ],
      "source": [
        "def parse_text_file(file_path: str):\n",
        "    \"\"\"\n",
        "    Reads and parses the contents of a text file (The entire text file content should be cleaned and stripped of unnecessary whitespace.\n",
        "    Make a list of dictionaries of messages where role is consistent (i.e \"user\") and content is each line (defined by \\n) of the given txt file without any escape characters).\n",
        "    Then calculates the tokens and cost.\n",
        "\n",
        "    Args:\n",
        "    - file_path (str): The path to the text file.\n",
        "\n",
        "    Returns:\n",
        "    - tuple[int, int]: A tuple containing total tokens and the cost (tokens, cost).\n",
        "    \"\"\"\n",
        "    f = open(file_path, \"r\")\n",
        "    text=f.read()\n",
        "    text=text.strip()\n",
        "    messages = [{\"role\": \"user\", \"content\": text}]\n",
        "    tokens, cost = count_tokens_and_cost_with_messages(messages).values()\n",
        "    return tokens, cost\n",
        "\n",
        "tokens, cost = parse_text_file(\"./sample.txt\")\n",
        "print(f\"Total Tokens: {tokens}\")\n",
        "print(f\"Estimated Cost: ${cost}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwlGz3UTqWX4",
        "outputId": "50000337-9f3c-4eab-cf53-c2b2b44ca44e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9wMl3hzfTHB",
        "outputId": "ea040dc9-33db-4c71-92e7-d1b5bd668e62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens: 188\n",
            "Estimated Cost: $0.000470\n"
          ]
        }
      ],
      "source": [
        "from pypdf import PdfReader\n",
        "def parse_pdf_file(pdf_path: str):\n",
        "    \"\"\"\n",
        "    Extracts and parses text from a PDF file (The entire text file content should be cleaned and stripped of unnecessary whitespace.\n",
        "    Make a list of dictionaries of messages where role is consistent (i.e \"user\") and content is each line (defined by \\n) of the given pdf file without any escape characters).\n",
        "    Then calculates the tokens and cost.\n",
        "\n",
        "    Args:\n",
        "    - pdf_path (str): The path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "    - tuple[int, int]: A tuple containing total tokens and the cost (tokens, cost).\n",
        "    \"\"\"\n",
        "    # Use PyPDF2 (or an equivalent library) to extract text from the PDF\n",
        "    # Normalize text by stripping extra spaces and handling line breaks\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text=\"\"\n",
        "    for page in reader.pages:\n",
        "        new_text = page.extract_text()\n",
        "        new_text.strip()\n",
        "        text+=new_text\n",
        "    messages = [{\"role\": \"user\", \"content\": text}]\n",
        "    result = count_tokens_and_cost_with_messages(messages)\n",
        "    tokens=result[\"Total Tokens\"]\n",
        "    cost=result[\"Cost (USD)\"]\n",
        "    return tokens, cost\n",
        "\n",
        "tokens, cost = parse_pdf_file(\"./sample.pdf\")\n",
        "print(f\"Total Tokens: {tokens}\")\n",
        "print(f\"Estimated Cost: ${cost:.6f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Counting Tokens in Harry Potter Books\n",
        "Building on what we did in extracting text from pdf, let's extract text from pdfs of Harry Potter Books. I have downloaded Harry Potter books for this purpose and will use function for counting tokens from pdfs to calculate tokens in Harry Potter Books."
      ],
      "metadata": {
        "id": "svJYb1Mmq2MK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "\n",
        "def tokens_in_Harry_Potter_Books():\n",
        "   os.chdir(\"./\")\n",
        "   pdfs = []\n",
        "   total_tokens=0\n",
        "   total_cost=0\n",
        "   for file in glob.glob(\"*.pdf\"):\n",
        "    print(file)\n",
        "    tokens,cost=parse_pdf_file(file)\n",
        "    total_tokens+=tokens\n",
        "    total_cost+=cost\n",
        "   return total_tokens,total_cost\n",
        "\n",
        "tokens, cost=tokens_in_Harry_Potter_Books()\n",
        "print(f\"Total Tokens: {tokens}\")\n",
        "print(f\"Estimated Cost: ${cost:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0iK7O_kvx1W",
        "outputId": "40d4d7e4-6817-460b-846f-d7e634663c66"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-harry-potter-and-the-order-of-the-phoenix.pdf\n",
            "3-Harry Potter And The Prisoner Of Azkabaan.pdf\n",
            "2-harry-potter-chamber-of-secrets.pdf\n",
            "1-harry-potter-sorcerers-stone.pdf\n",
            "7-harry-potter-and-the-deathly-hallows.pdf\n",
            "4-harry-potter-and-the-goblet-of-fire.pdf\n",
            "6-harry-potter-and-the-half-blood-prince.pdf\n",
            "Total Tokens: 1462704\n",
            "Estimated Cost: $3.656760\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}